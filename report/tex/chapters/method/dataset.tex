\section{Dataset}
    \paragraph{}The dataset at the core of this thesis, as previously introduced (see \ref{seq:background:dataset}), consists of heap dump raw files related to different OpenSSH use cases and versions. Each heap dump file is paired with a JSON annotation file created by the dataset's creators. These JSON files provide extra information about the heap dump, especially regarding encryption keys. In this section, we will explain our exploration of the dataset, aiming to better comprehend its content and nuances.

    \subsection{Origin}
        \paragraph{}The dataset is derived from heap dumps that capture various OpenSSH usage scenarios. These scenarios encompass four distinct SSH interactions: a straightforward client connection to the server followed by an immediate exit, port-forwarding, secure copying, and SSH shared connection. The heap dumps span different OpenSSH versions and a range of key sizes, from 16 to 64 bytes. These dumps were generated using the SmartKex tool \cite{fellicious_smartkex_2022}. The data collection was conducted on a mini PC equipped with an AMD Ryzen 5500U processor, 16GB of RAM, and a 1TB NVMe SSD, running Debian 11 as its operating system.

    \subsection{Estimating the dataset balancing for key prediction}
        \paragraph{}In this part, our primary objective was to assess the balance of the dataset for key prediction and identify the challenges associated with it.

        \paragraph{}To begin, we aimed to gain an understanding of the dataset's scale. We utilized a code snippet \ref{methods:code:count_all_dataset_files} to count all the files within the dataset, revealing a total of 208,745 files. However, it was imperative to recognize that JSON files, which served as annotation files, were not to be considered part of the raw bytes for embedding. Consequently, these JSON files were excluded from our count to provide a more accurate representation of the dataset's size.

        \begin{lstlisting}[caption={Count all dataset files}, label=methods:code:count_all_dataset_files, language=bash]
        find . -type f | wc -l
        \end{lstlisting}

        \paragraph{}Following this, we employed another code snippet \ref{methods:code:count_raw_files} to specifically count the heap dump raw files, excluding JSON files. This count indicated a total of 103,595 heap dump raw files, which constituted the primary focus of our analysis.

        \begin{lstlisting}[caption={Count heap dump raw dataset files}, label=methods:code:count_raw_files, language=bash]
        find . -type f -name "*.raw" | wc -l
        \end{lstlisting}

        \paragraph*{}To gain further insights into the dataset, we determined its size while excluding annotation files \ref{methods:code:get_dataset_size}. The calculated dataset size amounted to 18,067,001,344 bytes.

        \begin{lstlisting}[caption={Get the size of the dataset}, label=methods:code:get_dataset_size, language=bash]
        find . -type f -name "*.raw" -exec du -b {} + | awk '{s+=$1} END {print s}'
        \end{lstlisting}

        \paragraph{}Considering the nature of the dataset, which featured a maximum of six keys per file, each with a maximum size of 64 bytes, we conducted a rough estimate. We determined that the maximum number of bytes relevant for searching across the dataset was $6 * 64 * 103595 = 39 780 480$ . This calculation accounted for approximately 0.22\% of the dataset's total size.

        \paragraph{}Lastly, it is crucial to acknowledge that the dataset exhibited a significant imbalance and is very large. To address this challenge effectively, strategies were implemented to ensure robust, unbiased analyses, and scalability.
    \subsection*{Annotations}
        \paragraph{}The annotations files are essential to understand the data and how best to utilize them for the study. Each heap dump corresponds to one specific JSON file. To view the contents of these JSON files in a more organized manner, one can reference the method provided at \ref{methods:code:pretty_print_json}. For a clearer understanding, an extract of the JSON annotation from the file located at \path{./Training/client/V_7_8_P1/16/13116-1644920217.json} is available at \ref{methods:code:annotation_extract}.

        \begin{lstlisting}[caption={pretty print JSON}, label=methods:code:pretty_print_json, language=bash]
            python3 -m json.tool file.json
        \end{lstlisting}

        \noindent
        \begin{minipage}{\linewidth}
        \begin{lstlisting}[language=json, caption={An extract of the JSON annotations}, label=methods:code:annotation_extract]
        {
            /* file ./Training/client/V_7_8_P1/16/13116-1644920217.json*/
            "SSH_STRUCT_ADDR": "5619dd7e5570",
            "SESSION_STATE_ADDR": "5619dd7e5df0",
            "KEY_A_ADDR": "5619dd807f40",
            "KEY_A_LEN": "12",
            "KEY_A_REAL_LEN": "12",
            "KEY_A": "34fbe182e76c49a617a93e2e",
            /*...*/
            "KEY_E_ADDR": "5619dd808000",
            "KEY_E_LEN": "0",
            "KEY_E_REAL_LEN": "0",
            "KEY_E": "",
            "KEY_F_ADDR": "5619dd807fd0",
            "KEY_F_LEN": "0",
            "KEY_F_REAL_LEN": "0",
/            "KEY_F": "",
            "HEAP_START": "5619dd7e3000"
        }
        \end{lstlisting}
        \end{minipage}

        \paragraph{}Within these annotation files, several critical pieces of information are present. The ``SSH\_STRUCT\_ADDR'' and ``SESSION\_STATE\_ADDR'' denote the addresses of vital openSSH structures. These addresses are pivotal in gauging the embedding coherence across different openSSH uses and key sizes. If the embeddings of these structures display similarity across various key sizes and openSSH usages, it signifies the embedding's coherence.

        \paragraph{}Other significant annotations such as ``KEY\_A\_ADDR'', ``KEY\_A\_LEN'', ``KEY\_A\_REAL\_LEN'', and ``KEY\_A'' detail the address, length, and value of the key A. In general, six of these annotations can be found for each heap dump. Notably, the ``HEAP\_START'' annotation, along with the length of the heap dump, is of paramount importance. This annotation signifies the starting address of the heap dump. This information not only aids in pinpointing addresses in the heap dump for structures and \glspl{pointer}, but also refines the heuristic used in detecting \glspl{pointer}. By leveraging the ``HEAP\_START'' information, one can verify if a \gls{pointer} is pointing within the heap dump boundaries. As a practical illustration, deducing the address of key A within the heap dump can be achieved by subtracting ``HEAP\_START'' from ``KEY\_A\_ADDR''.

        \paragraph{}However, it's noteworthy that some of these annotation files may be corrupted. Therefore, it's imperative to verify the integrity of each file before its use. In instances where keys are corrupted, such as "KEY\_E" and "KEY\_F" having no recorded values in the extract found at \ref{methods:code:annotation_extract}, it's advised either to remove the corrupted keys or discard the entire file if the data cannot be salvaged.
    
    \subsection{Dataset Validation}
        \paragraph{}The dataset primarily consists of heap dump RAW files, each corresponding to various use cases and versions of OpenSSH. Accompanying each heap dump is a JSON annotation file, crafted by the dataset's creators, to furnish supplementary details, particularly about encryption keys.
        
        \paragraph{}However, the dataset isn't without its flaws. Its application in machine learning has unveiled certain inconsistencies. For example, a few of these files are incomplete, lacking essential data. This poses a challenge since we rely on these annotations to pinpoint key addresses, crucial for annotating memory graphs in the embedding phase. If there's a discrepancy in the format, we'll deem the JSON annotation as corrupted and bypass it. This likely stems from the automated generation of annotations. A case in point is the file in \textit{Training/basic/V\_7\_8\_P1/16/13116-1644920217.json}, which, being the dataset's first file, showcases an incomplete annotation with absent keys. It's vital to be cognizant of these limitations when utilizing the dataset for academic endeavors.
        
        \subsubsection{Annotation Integrity Verification}
            \paragraph{}To accurately gauge the usability of the dataset for machine learning applications, we implemented a validation script named \texttt{check\_annotations.py}. This script is tailored to assess the annotations for their quality, completeness, and consistency.

            \paragraph{}The annotations (JSON files) are categorized as follows:
            \begin{itemize}
                \item \textbf{Complete and Accurate Files}: These files are devoid of missing keys and contain all keys with appropriate values.
                \item \textbf{Malformed Files}: These are files that aren't valid JSON and hence cannot be loaded properly.
                \item \textbf{Inconsistent Files}: Files that present conflicting information within their annotations.
                \item \textbf{Files with Absent Keys}: These files lack certain keys in their annotations. For instance, a JSON file might have "KEY\_E": "", indicating the absence of key E and its corresponding address in the annotation, which poses challenges for accurate machine learning labeling.
                \item \textbf{Files with Incomplete Keys}: These files contain keys but lack the corresponding addresses. An example would be a JSON file with "KEY\_E": "689e549a80ce4be95d8b742e36a229bf", signifying the presence of key E but the absence of its address in the annotation. This again complicates the labeling process for machine learning.
            \end{itemize}

            \paragraph{}The script executes swiftly, processing all the 103,595 JSON annotation files and yields the following outcomes:

            \begin{itemize}
                \item \textbf{Correct and Complete Files}: 26,202 files.
                \item \textbf{Broken Files}: 6 files are identified as broken. Closer inspection reveals these files to be empty.
                \item \textbf{Incorrect Files}: 0 files.
                \item \textbf{Files with Absent Keys}: 58,643 files exhibit missing keys.
                \item \textbf{Files with Incomplete Keys}: 18,750 files display incomplete keys.
            \end{itemize}

            \paragraph{}Delving deeper into the keys:

            \begin{itemize}
                \item \textbf{Total SSH Keys}: 546,534 keys.
                \item \textbf{Missing (Empty) SSH Keys}: 157,244 keys.
                \item \textbf{Incompletely Annotated SSH Keys}: 37,500 keys.
                \item \textbf{Incorrectly Annotated SSH Keys}: 0 keys.
            \end{itemize}

    \subsection{Structure of the Heap File}
        \paragraph{}Heap files serve as the dynamic memory storage for applications, and understanding their structure is crucial for memory analysis. These files are organized in a specific manner, with memory sequences of bytes or "chunks" allocated and deallocated as needed by the application. The heap is 8-byte aligned, which means that we can consider sequences of memory in 8-byte \gls{block}. This alignment ensures efficient memory access and management. To visualize and interpret the heap's structure, tools like memory analyzers or debuggers can be employed.

        \paragraph{}Within the heap, there are four primary types of byte sequences that can be identified with varying degrees of certainty:
        
        \begin{enumerate}
            \item \textbf{Chunk, Malloc Header, and Footer}: We have already discussed these components in detail in an earlier section. In brief, they represent the primary building blocks of the heap, with each chunk being a segment of memory allocated for storing data. The malloc header contains essential metadata about the chunk, and footers, when present, replicate this information.
            \item \textbf{Pointer}: Memory addresses that reference other locations within the heap or other memory segments.
        \end{enumerate}
        
        \paragraph{}Any unidentified user data within these structures is termed as "value data." This data represents the actual content or payload stored within the allocated memory chunks.

        \subsubsection{Chunk}
        \paragraph{}In our exploration of the dataset, the chunk chaining assumption, as detailed in section \ref{seq:background:chunk_chaining}, plays a pivotal role. It's imperative to ensure the integrity of this assumption for accurate analysis. During the dataset refinement process, we identified five heap dumps that contain chunks with a size of 0 bytes, which could potentially violate this assumption. To maintain the reliability of our analyses, these specific dumps have been removed from the dataset. 
        
        \subsubsection{Pointer}\label{seq:methods:dataset:pointer}
            \paragraph{}Pointers are memory addresses that reference other locations within the heap or other segments of memory. In the context of the heap, pointers can indicate data structures, reference other chunks, or provide links in data structures like linked lists or trees. To identify potential pointers within the heap dump, one can utilize the following Reggex :
            \begin{lstlisting}
            1 :/[0-9a-f]\{12}0\{4}
            \end{lstlisting}

            \paragraph{}This command searches for sequences comprising 12 hexadecimal characters succeeded by 4 zeros. The rationale behind this is twofold: 
            \begin{itemize}
                \item The heap dump file's maximum possible addresses typically span around 12 hexadecimal digits.
                \item Pointers' addresses are represented in little-endian format. Consequently, the address's last 4 bytes at 0 are its Most Significant Bytes (MSB).
            \end{itemize}
            \paragraph{}Furthermore, with knowledge of the heap's start addresses and the dump's size, we can enhance the precision of our search. By doing so, we can exclude potential pointers that point outside the boundaries of the dump. Another refinement can be made by verifying if the values pointed to by the potential pointers are 8 bytes aligned, as the heap is structured in 8-byte sequences. However, it's crucial to note that this approach remains heuristic in nature. As such, there's still a possibility of detecting blocks that aren't genuine pointers.
        \subsubsection{Footer}
            \paragraph{}In the GLIBC documentation, the footer of a chunk is expected to mirror the chunk's size as indicated in the malloc header. However, an inconsistency is observed: the size stated in the footer block doesn't always align with this expectation. This deviation is consistently seen across the refined dataset.
    

        

    \subsection{Heap File Distribution}

        
            
        \paragraph{}The dataset offers an in-depth perspective on heap dumps, systematically sorted by key size, OpenSSH version, and distinct use cases. This methodical arrangement streamlines the analytical process, enabling precise investigations tailored to particular criteria. In the subsequent sections, we'll delve into the distribution of the dataset, emphasizing the file count across different use cases, versions, and key sizes, to ensure its suitability for consistent testing.

        
        \subsubsection{Full Dataset}
            \paragraph{}In our initial phase of exploration, we will concentrate on the full dataset. This comprehensive analysis will provide a holistic understanding of the data's structure, variations, and potential anomalies.



            \begin{itemize}
                \item A unique instance was observed where a folder was devoid of any content. This was in the Training section, specifically for the client use case, version V\_7\_8\_P1, and a key size of 64 bytes.
                \item In the Training segment, which comprises 82 combinations of use cases, versions, and key sizes:
                \begin{itemize}
                    \item The minimum number of RAW files present is 923.
                    \item The maximum stretches to 1095. The difference between these two extremes is calculated as:
                    \begin{equation}
                        \frac{\text{max} - \text{min}}{\text{min}} = 0.186
                    \end{equation}
                    This results in a 18.6\% difference.
                \end{itemize}
                \item For the Testing segment, which has 15 combinations:
                \begin{itemize}
                    \item The RAW files range from a minimum of 100 to a maximum of 101, marking a mere 1\% difference between the two.
                \end{itemize}
                \item The Validation segment, with its 82 combinations, shows:
                \begin{itemize}
                    \item A minimum of 151 RAW files.
                    \item A maximum of 211 RAW files. The difference between these values is:
                    \begin{equation}
                        \frac{\text{max} - \text{min}}{\text{min}} = 0.397
                    \end{equation}
                    This presents a 39.7\% difference, yet the number of files remains substantial enough to validate any model effectively.
                \end{itemize}
            \end{itemize}
            
            \paragraph{}The dataset, with its meticulous organization and vast range, offers a robust platform for in-depth analysis and model validation.
            
        \subsubsection{Clean Dataset}
            \paragraph{}Following our examination of the full dataset, we will shift our focus to the cleaned dataset. This refined subset, having undergone meticulous preprocessing and filtering, will offer insights into the most pertinent and reliable data points. Analyzing the cleaned dataset will ensure that our conclusions and subsequent actions are based on high-quality, accurate data.
            
            \begin{itemize}
                \item In the Training segment, which comprises 82 combinations of use cases, versions, and key sizes:
                
                \begin{itemize}
                    \item 63 subdirectories are empty, with no RAW files present.
                    \item The minimum number of RAW files present is 923.
                    \item The maximum stretches to 1079. The difference between these two extremes is calculated as:
                    \begin{equation}
                        \frac{\text{max} - \text{min}}{\text{min}} = 0.169
                    \end{equation}
                    This results in a 16.9\% difference.
                \end{itemize}
                \item For the Testing segment, which had 15 combinations : 0 subdirectories are empty, then no changes are observed.
                \item The Validation segment, with its 82 combinations, shows:
                \begin{itemize}
                    \item 62 subdirectories are empty, with no RAW files present.
                    \item A minimum of 151 RAW files.
                    \item A maximum of 209 RAW files. The difference between these values is:
                    \begin{equation}
                        \frac{\text{max} - \text{min}}{\text{min}} = 0.384
                    \end{equation}
                    This presents a 38.4\% difference, yet the number of files remains substantial enough to validate any model effectively.
                \end{itemize}
            \end{itemize}
            
            \paragraph{}The specifics of the empty folders, including their exact locations and other details, will be cataloged comprehensively in the annex~\ref{sec:annexes:dataset_cleaning_results}. It's crucial to note that due to the invalid nature of the data in these folders, our coherence study on the embeddings will not factor in the OpenSSH version, use case, or key size involved. This decision ensures that our analysis remains rooted in valid and meaningful data, thereby enhancing the reliability of our findings.

            \paragraph{}While the cleaning process did invalidate certain cases within the dataset, it's essential to emphasize that a significant portion remains intact and consequential. These preserved cases provide a robust foundation for our analysis, ensuring that our study is both comprehensive and grounded in meaningful data. The invalidated cases, though notable, do not diminish the overall value and depth of the dataset at our disposal.
    
    \subsection{Keys Analysis}\label{sec:methods:keys_analysis}

        \paragraph{}The analysis of SSH keys within the heap dumps provides crucial insights into their characteristics and behaviors. These findings not only enhance our understanding of the data but also guide subsequent steps in the research process.
    
        \subsubsection{Keys Positions}
    
        \paragraph{}Upon analyzing all the heap dumps, it became evident that all the SSH keys mentioned in the annotations are positioned at the beginning of their respective chunks. These keys have a size ranging from a minimum of 12 bytes to a maximum of 64 bytes. Additionally, the size of the chunks in which these keys are found is consistently observed to be 32, 48, or 64 bytes. This consistent positioning and size uniformity greatly simplify certain embedding processes and offer a streamlined approach to further analysis.

        \subsubsection{Keys Entropy}
    
            \paragraph{}Another significant observation regarding the keys is their high entropy, as referenced in~\ref{sec:related_work:smartkex}. High entropy is indicative of randomness, which is a characteristic feature of cryptographic keys. Leveraging this high entropy~\ref{eq:shannon_entropy} can be instrumental in discriminating the keys from other data. However, it's essential to approach this method with caution. While high entropy can be a strong indicator, it's not foolproof. There's a possibility of encountering false positives, as other high entropy data might exist in the heap. Additionally, there might be instances of false negatives, especially when keys contain multiple repeated bytes by chance.
    