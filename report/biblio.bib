
@inproceedings{laaksonen_classification_1996,
	location = {Washington, {DC}, {USA}},
	title = {Classification with learning k-nearest neighbors},
	volume = {3},
	isbn = {978-0-7803-3210-2},
	url = {http://ieeexplore.ieee.org/document/549118/},
	doi = {10.1109/ICNN.1996.549118},
	abstract = {The nearest neighbor ({NN}) classifiers, especially the k-{NN} algorithm, are among the simplest and yet most efficient classification rules and are widely used in practice. We introduce three adaptation rules that can be used in iterative training of a k-{NN} classifier. This is a novel approach both from the statistical pattern recognition and the supervised neural network learning points of view. The suggested learning rules resemble those of the well-known Learning Vector Quantization ({LVQ}) method, but at the same time the classifier utilizes the fact that increasing the number of samples that the classification is based on leads to improved classification accuracy. The performances of the suggested learning rules are compared with the usual k-{NN} rules and the {LVQl} algorithm.},
	eventtitle = {International Conference on Neural Networks ({ICNN}'96)},
	pages = {1480--1483},
	booktitle = {Proceedings of International Conference on Neural Networks ({ICNN}'96)},
	publisher = {{IEEE}},
	author = {Laaksonen, J. and Oja, E.},
	urldate = {2023-08-30},
	date = {1996},
	langid = {english},
	file = {Laaksonen et Oja - 1996 - Classification with learning k-nearest neighbors.pdf:/home/clement/Zotero/storage/VKKSVUW6/Laaksonen et Oja - 1996 - Classification with learning k-nearest neighbors.pdf:application/pdf},
}

@article{wu_analysis_2006,
	title = {Analysis of Support Vector Machine Classiﬁcation},
	volume = {8},
	abstract = {This paper studies support vector machine classiﬁcation algorithms. We analyze the 1-norm soft margin classiﬁer. The consistency is considered in two forms. When the regularization error decays to zero, the Bayes-risk consistency is proved and learning rates are derived by means of techniques of uniform convergence. The main diﬃculty we overcome here is to bound the oﬀset. For the consistency with hypothesis space, we present a counterexample.},
	number = {2},
	journaltitle = {Journal of Computational Analysis \& Applications},
	author = {Wu, Qiang and Zhou, Ding-Xuan},
	date = {2006},
	langid = {english},
	file = {Wu et Zhou - Analysis of Support Vector Machine Classiﬁcation.pdf:/home/clement/Zotero/storage/3E5TTXSF/Wu et Zhou - Analysis of Support Vector Machine Classiﬁcation.pdf:application/pdf},
}

@article{probst_hyperparameters_2019,
	title = {Hyperparameters and Tuning Strategies for Random Forest},
	volume = {9},
	issn = {1942-4787, 1942-4795},
	url = {http://arxiv.org/abs/1804.03515},
	doi = {10.1002/widm.1301},
	abstract = {The random forest algorithm ({RF}) has several hyperparameters that have to be set by the user, e.g., the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain and the number of trees. In this paper, we ﬁrst provide a literature review on the parameters’ inﬂuence on the prediction performance and on variable importance measures.},
	pages = {e1301},
	number = {3},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	shortjournal = {{WIREs} Data Min \& Knowl},
	author = {Probst, Philipp and Wright, Marvin and Boulesteix, Anne-Laure},
	urldate = {2023-08-30},
	date = {2019-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.03515 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Probst et al. - 2019 - Hyperparameters and Tuning Strategies for Random F.pdf:/home/clement/Zotero/storage/ATYXZF23/Probst et al. - 2019 - Hyperparameters and Tuning Strategies for Random F.pdf:application/pdf},
}

@article{kotsiantis_decision_2013,
	title = {Decision trees: a recent overview},
	volume = {39},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-011-9272-4},
	doi = {10.1007/s10462-011-9272-4},
	shorttitle = {Decision trees},
	abstract = {Decision tree techniques have been widely used to build classiﬁcation models as such models closely resemble human reasoning and are easy to understand. This paper describes basic decision tree issues and current research points. Of course, a single article cannot be a complete review of all algorithms (also known induction classiﬁcation trees), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
	pages = {261--283},
	number = {4},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Kotsiantis, S. B.},
	urldate = {2023-08-30},
	date = {2013-04},
	langid = {english},
	file = {Kotsiantis - 2013 - Decision trees a recent overview.pdf:/home/clement/Zotero/storage/94EWYUZR/Kotsiantis - 2013 - Decision trees a recent overview.pdf:application/pdf},
}

@collection{ambrosius_topics_2007,
	location = {Totowa, N.J},
	title = {Topics in biostatistics},
	isbn = {978-1-58829-531-6},
	series = {Methods in molecular biology},
	abstract = {Presents a multidisciplinary survey of biostatics methods, each illustrated with hands-on examples. Methods range from the elementary, including descriptive statistics, study design, statistical interference, categorical variables, evaluation of diagnostic tests, comparison of means, linear regression, and logistic regression. These introductory methods create a portfolio of biostatistical techniques for both novice and expert researchers. More complicated statistical methods are introduced as well, including those requiring either collaboration with a biostatistician or the use of a statistical package. Specific topics of interest include microarray analysis, missing data techniques, power and sample size, statistical methods in genetics.--},
	pagetotal = {528},
	number = {404},
	publisher = {Humana Press},
	editor = {Ambrosius, Walter T.},
	date = {2007},
	langid = {english},
	note = {{OCLC}: ocn159977868},
	keywords = {Aufsatzsammlung, Biometry, Biostatistik, Informatics, Methodology, methods},
	file = {Ambrosius - 2007 - Topics in biostatistics.pdf:/home/clement/Zotero/storage/T7A2ISBW/Ambrosius - 2007 - Topics in biostatistics.pdf:application/pdf},
}

@article{nick_logistic_2007,
	title = {Logistic regression},
	pages = {273--301},
	journaltitle = {Topics in biostatistics},
	author = {Nick, Todd G and Campbell, Kathleen M},
	date = {2007},
	note = {Publisher: Springer},
	file = {Nick et Campbell - 2007 - Logistic regression.pdf:/home/clement/Zotero/storage/NS5FEUBX/Nick et Campbell - 2007 - Logistic regression.pdf:application/pdf},
}

@article{ramyachitra_imbalanced_2014,
	title = {{IMBALANCED} {DATASET} {CLASSIFICATION} {AND} {SOLUTIONS}: A {REVIEW}},
	volume = {5},
	abstract = {Imbalanced data set problem occurs in classification, where the number of instances of one class is much lower than the instances of the other classes. The main challenge in imbalance problem is that the small classes are often more useful, but standard classifiers tend to be weighed down by the huge classes and ignore the tiny ones. In machine learning the imbalanced datasets has become a critical problem and also usually found in many applications such as detection of fraudulent calls, bio-medical, engineering, remote-sensing, computer society and manufacturing industries. In order to overcome the problems several approaches have been proposed. In this paper a study on Imbalanced dataset problem and the solution is given.},
	number = {4},
	journaltitle = {International Journal of Computing and Business Research},
	author = {Ramyachitra, Dr D and Manikandan, P},
	date = {2014},
	langid = {english},
	file = {Ramyachitra et Manikandan - 2014 - IMBALANCED DATASET CLASSIFICATION AND SOLUTIONS A.pdf:/home/clement/Zotero/storage/7GAZ68WX/Ramyachitra et Manikandan - 2014 - IMBALANCED DATASET CLASSIFICATION AND SOLUTIONS A.pdf:application/pdf},
}

@online{noauthor_certcc_nodate,
	title = {{CERT}/{CC} Vulnerability Note {VU}\#13877},
	url = {https://www.kb.cert.org},
	abstract = {Weak {CRC} allows packet injection into {SSH} sessions encrypted with block ciphers},
	urldate = {2023-08-30},
}

@inproceedings{yurcik_first_2005,
	title = {A first step toward detecting {SSH} identity theft in {HPC} cluster environments: discriminating masqueraders based on command behavior},
	volume = {1},
	doi = {10.1109/CCGRID.2005.1558542},
	shorttitle = {A first step toward detecting {SSH} identity theft in {HPC} cluster environments},
	abstract = {Recent attacks enabled by stolen authentication passwords and keys have allowed intruders to masquerade as legitimate users on high performance computing clusters. With the motivation of detecting masqueraders on clusters, this work seeks to discriminate different types of users based on their command behavior - in particular, user command behavior on a multi-user public machine versus user command behavior on a high performance computing cluster. Our intuition is that these users act differently and the unique high performance cluster environment is constrained such that command behavior discrimination is enhanced versus enterprise environments. We formalize this into a classification problem to be solved by a support vector machine with {TF}-{IDF} feature construction techniques from the field of Information Retrieval. We present results showing the effectiveness of this approach exhibiting high precision depending on the length of monitoring in both time and number of commands. In particular we show that as few as 10 commands may be enough to recognize a masquerading attacker on a high performance computing cluster.},
	eventtitle = {{CCGrid} 2005. {IEEE} International Symposium on Cluster Computing and the Grid, 2005.},
	pages = {111--120 Vol. 1},
	booktitle = {{CCGrid} 2005. {IEEE} International Symposium on Cluster Computing and the Grid, 2005.},
	author = {Yurcik, W. and Liu, Chao},
	date = {2005-05},
	keywords = {Authentication, Chaos, High performance computing, Information retrieval, Inspection, Monitoring, Performance analysis, Security, Support vector machine classification, Support vector machines},
}

@article{koppen_curse_2000,
	title = {The curse of dimensionality},
	volume = {1},
	abstract = {In this text, some question related to higher dimensional geometrical spaces will be discussed. The goal is to give the reader a feeling for geometric distortions related to the use of such spaces (e.g. as search spaces).},
	pages = {4--8},
	author = {Koppen, Mario},
	date = {2000},
	langid = {english},
	file = {Koppen - The curse of dimensionality.pdf:/home/clement/Zotero/storage/983R36QZ/Koppen - The curse of dimensionality.pdf:application/pdf},
}

@incollection{hutchison_curse_2005,
	location = {Berlin, Heidelberg},
	title = {The Curse of Dimensionality in Data Mining and Time Series Prediction},
	volume = {3512},
	isbn = {978-3-540-26208-4 978-3-540-32106-4},
	url = {http://link.springer.com/10.1007/11494669_93},
	pages = {758--770},
	booktitle = {Computational Intelligence and Bioinspired Systems},
	publisher = {Springer Berlin Heidelberg},
	author = {Verleysen, Michel and François, Damien},
	editor = {Cabestany, Joan and Prieto, Alberto and Sandoval, Francisco},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2023-08-30},
	date = {2005},
	doi = {10.1007/11494669_93},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{khalid_survey_2014,
	title = {A survey of feature selection and feature extraction techniques in machine learning},
	doi = {10.1109/SAI.2014.6918213},
	abstract = {Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.},
	eventtitle = {2014 Science and Information Conference},
	pages = {372--378},
	booktitle = {2014 Science and Information Conference},
	author = {Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
	date = {2014-08},
	keywords = {Accuracy, Age Related Macula Degeneration ({AMD}), Algorithm design and analysis, Correlation, Correlation Based Method, Feature extraction, Feature Extraction/Transformation, Feature Selection, Feature Subset Selection, {FSA}'s, {ICA}, Noise, {PCA}, Principal component analysis, Redundancy, {RELIEF}},
	file = {IEEE Xplore Abstract Record:/home/clement/Zotero/storage/9ILHCP4W/6918213.html:text/html;IEEE Xplore Full Text PDF:/home/clement/Zotero/storage/M34E8V5U/Khalid et al. - 2014 - A survey of feature selection and feature extracti.pdf:application/pdf},
}

@misc{khurana_feature_2017,
	title = {Feature Engineering for Predictive Modeling using Reinforcement Learning},
	url = {http://arxiv.org/abs/1709.07150},
	abstract = {Feature engineering is a crucial step in the process of predictive modeling. It involves the transformation of given feature space, typically using mathematical functions, with the objective of reducing the modeling error for a given target. However, there is no well-deﬁned basis for performing effective feature engineering. It involves domain knowledge, intuition, and most of all, a lengthy process of trial and error. The human attention involved in overseeing this process signiﬁcantly inﬂuences the cost of model generation. We present a new framework to automate feature engineering. It is based on performance driven exploration of a transformation graph, which systematically and compactly enumerates the space of given options. A highly efﬁcient exploration strategy is derived through reinforcement learning on past examples.},
	number = {{arXiv}:1709.07150},
	publisher = {{arXiv}},
	author = {Khurana, Udayan and Samulowitz, Horst and Turaga, Deepak},
	urldate = {2023-08-30},
	date = {2017-09-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.07150 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Khurana et al. - 2017 - Feature Engineering for Predictive Modeling using .pdf:/home/clement/Zotero/storage/R5YW6N45/Khurana et al. - 2017 - Feature Engineering for Predictive Modeling using .pdf:application/pdf},
}

@article{martinez_garre_novel_2021,
	title = {A novel Machine Learning-based approach for the detection of {SSH} botnet infection},
	volume = {115},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X20303265},
	doi = {10.1016/j.future.2020.09.004},
	abstract = {Botnets are causing severe damages to users, companies, and governments through information theft, abuse of online services, {DDoS} attacks, etc. Although significant research is being made to detect them and mitigate their effect, they are exponentially increasing due to new zero-day attacks, a variation of their behavior, and obfuscation techniques. High Interaction Honeypots ({HIH}) are the only honeypots able to capture attacks and log all the information generated by attackers when setting up a botnet. The data generated is being processed using Machine Learning ({ML}) techniques for detection since they can detect hidden patterns. However, so far, research has been focused on intermediate phases of the botnet’s life cycle during operation, underestimating the initial phase of infection. To the best of our knowledge, this is the first solution in the infection phase of {SSH}-based botnets. Therefore, we have designed an approach based on an {SSH}-based {HIH} to generate a dataset consisting of executed commands and network information. Herein, we have applied {ML} techniques for the development of a real-time detection model. This approach reached a very high level of prediction and zero false negatives. Indeed, our system detected all known and unknown {SSH} sessions intended to infect our honeypots. Thus, our research has demonstrated that new {SSH} infections can be detected through {ML} techniques.},
	pages = {387--396},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Martínez Garre, José Tomás and Gil Pérez, Manuel and Ruiz-Martínez, Antonio},
	urldate = {2023-08-30},
	date = {2021-02-01},
	keywords = {Botnet, High interaction, Honeypot, Machine learning, Zero-day malware},
}

@article{hinton_reducing_2006,
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1127647},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	pages = {504--507},
	number = {5786},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	urldate = {2023-08-30},
	date = {2006-07-28},
	langid = {english},
	file = {Hinton et Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf:/home/clement/Zotero/storage/JLNIYY7J/Hinton et Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf:application/pdf},
}

@article{wheeler_problems_2011,
	title = {Problems with Skewness and Kurtosis},
	journaltitle = {Quality Digest Daily},
	author = {Wheeler, Donald J},
	date = {2011-08-01},
	langid = {english},
	file = {Wheeler - Problems with Skewness and Kurtosis.pdf:/home/clement/Zotero/storage/2WCWNHPG/Wheeler - Problems with Skewness and Kurtosis.pdf:application/pdf},
}

@article{cain_univariate_2017,
	title = {Univariate and multivariate skewness and kurtosis for measuring nonnormality: Prevalence, influence and estimation},
	volume = {49},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-016-0814-1},
	doi = {10.3758/s13428-016-0814-1},
	shorttitle = {Univariate and multivariate skewness and kurtosis for measuring nonnormality},
	abstract = {Nonnormality of univariate data has been extensively examined previously (Blanca et al., Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 9(2), 78–84, 2013; Miceeri, Psychological Bulletin, 105(1), 156, 1989). However, less is known of the potential nonnormality of multivariate data although multivariate analysis is commonly used in psychological and educational research. Using univariate and multivariate skewness and kurtosis as measures of nonnormality, this study examined 1,567 univariate distriubtions and 254 multivariate distributions collected from authors of articles published in Psychological Science and the American Education Research Journal. We found that 74 \% of univariate distributions and 68 \% multivariate distributions deviated from normal distributions. In a simulation study using typical values of skewness and kurtosis that we collected, we found that the resulting type I error rates were 17 \% in a t-test and 30 \% in a factor analysis under some conditions. Hence, we argue that it is time to routinely report skewness and kurtosis along with other summary statistics such as means and variances. To facilitate future report of skewness and kurtosis, we provide a tutorial on how to compute univariate and multivariate skewness and kurtosis by {SAS}, {SPSS}, R and a newly developed Web application.},
	pages = {1716--1735},
	number = {5},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Cain, Meghan K. and Zhang, Zhiyong and Yuan, Ke-Hai},
	urldate = {2023-08-30},
	date = {2017-10},
	langid = {english},
	file = {Cain et al. - 2017 - Univariate and multivariate skewness and kurtosis .pdf:/home/clement/Zotero/storage/MVY7S7S9/Cain et al. - 2017 - Univariate and multivariate skewness and kurtosis .pdf:application/pdf},
}

@article{gehring_convolutional_2017,
	title = {Convolutional Sequence to Sequence Learning},
	url = {https://arxiv.org/pdf/1705.03122.pdf},
	journaltitle = {Facebook {AI} Research},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
	date = {2017-07-25},
	langid = {english},
	file = {Gehring et al. - Convolutional Sequence to Sequence Learning.pdf:/home/clement/Zotero/storage/RWL9YDL3/Gehring et al. - Convolutional Sequence to Sequence Learning.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-Based Learning Applied to Document Recognition},
	journaltitle = {proc of the {IEEE}},
	author = {{LeCun}, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
	date = {1998},
	langid = {english},
	file = {LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:/home/clement/Zotero/storage/JTT9JKFY/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:application/pdf},
}

@article{shannon_mathematical_1948,
	title = {A Mathematical Theory of Communication},
	volume = {27},
	pages = {379--423},
	journaltitle = {The Bell System Technical Journa},
	author = {Shannon, C E},
	date = {1948-10},
	langid = {english},
	file = {Shannon - A Mathematical Theory of Communication.pdf:/home/clement/Zotero/storage/39BJAWAH/Shannon - A Mathematical Theory of Communication.pdf:application/pdf},
}

@misc{chung_empirical_2014,
	title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks ({RNNs}). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory ({LSTM}) unit and a recently proposed gated recurrent unit ({GRU}). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found {GRU} to be comparable to {LSTM}.},
	number = {{arXiv}:1412.3555},
	publisher = {{arXiv}},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, {KyungHyun} and Bengio, Yoshua},
	urldate = {2023-08-23},
	date = {2014-12-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.3555 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:/home/clement/Zotero/storage/KI4CBV2F/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to Sequence Learning with Neural Networks},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}’14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}’s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difﬁculty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	number = {{arXiv}:1409.3215},
	publisher = {{arXiv}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	urldate = {2023-08-23},
	date = {2014-12-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.3215 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:/home/clement/Zotero/storage/VR9CW4R4/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2023-08-23},
	date = {1997},
	note = {Publisher: {MIT} Press},
	file = {Long short-term memory.pdf:/home/clement/Zotero/storage/U2UBGXGB/Long short-term memory.pdf:application/pdf},
}

@misc{bai_empirical_2018,
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as {LSTMs} across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/{TCN}.},
	number = {{arXiv}:1803.01271},
	publisher = {{arXiv}},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	urldate = {2023-08-23},
	date = {2018-04-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.01271 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:/home/clement/Zotero/storage/TM3UYTWZ/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	volume = {30},
	pages = {5998--6008},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-08-23},
	date = {2017},
	file = {Attention Is All You Need.pdf:/home/clement/Zotero/storage/QEQIBNKM/Attention Is All You Need.pdf:application/pdf},
}

@article{lai_recurrent_2015,
	title = {Recurrent Convolutional Neural Networks for Text Classification},
	volume = {29},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9513},
	doi = {10.1609/aaai.v29i1.9513},
	abstract = {Text classiﬁcation is a foundational task in many {NLP} applications. Traditional text classiﬁers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classiﬁcation without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classiﬁcation to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	urldate = {2023-08-23},
	date = {2015-02-19},
	langid = {english},
	file = {Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text C.pdf:/home/clement/Zotero/storage/CP2DU5RB/Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text C.pdf:application/pdf},
}

@article{hiester_file_2018,
	title = {File Fragment Classification Using Neural Networks with Lossless Representations},
	abstract = {This study explores the use of neural networks as universal models for classifying ﬁle fragments. This approach diﬀers from previous work in its lossless feature representation, with fragments’ bits as direct input, and its use of feedforward, recurrent, and convolutional networks as classiﬁers, whereas previous work has only tested feedforward networks. Due to the study’s exploratory nature, the models were not directly evaluated in a practical setting; rather, easily reproducible experiments were performed to attempt to answer the intial question of whether this approach is worthwhile to pursue further, especially due to its high computational cost. The experiments tested classiﬁcation of fragments of homogeneous ﬁle types as an idealized case, rather than using a realistic set of types, because the types of interest are highly application-dependent. The recurrent networks achieved 98 percent accuracy in distinguishing 4 ﬁle types, suggesting that this approach may be capable of yielding models with sufﬁcient performance for practical applications. The potential applications depend mainly on the model performance gains achievable by future work but include binary mapping, deep packet inspection, and ﬁle carving.},
	journaltitle = {East Tennessee State University},
	author = {Hiester, Luke},
	urldate = {2023-08-21},
	date = {2018-05},
	langid = {english},
	file = {Hiester - File Fragment Classification Using Neural Networks.pdf:/home/clement/Zotero/storage/J5IUYN87/Hiester - File Fragment Classification Using Neural Networks.pdf:application/pdf},
}

@article{sentanoe_sshkex_2022,
	title = {{SSHkex}: Leveraging virtual machine introspection for extracting {SSH} keys and decrypting {SSH} network traffic},
	volume = {40},
	issn = {26662817},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666281722000063},
	doi = {10.1016/j.fsidi.2022.301337},
	shorttitle = {{SSHkex}},
	abstract = {Nowadays, many users are using an encrypted channel to communicate with a remote resource server. Such a channel provides a high degree of privacy and conﬁdentiality. Secure Shell ({SSH}) is one of the most commonly used methods to connect to a server remotely. {SSH} provides privacy and conﬁdentiality by encrypting network trafﬁc between the client and the server. The encryption makes the learning process of malicious activities over {SSH} is challenging, especially by just analyzing the network trafﬁc. To overcome the problem, we can leverage Virtual machine introspection ({VMI}). {VMI} allows direct memory access of a virtual machine ({VM}) including accessing data of an {SSH} process. However, the current prototype suffers from high overhead since it extracts every single plain text {SSH} network payload from memory and the extraction process requires the virtual machine ({VM}) to be momentarily paused. In this paper, we introduce {SSHkex}, a tool that also leverages {VMI} to extracts {SSH}'s session keys from a server's memory. Our approach only needs to pause the {VM} twice to extract the session keys for each {SSH} session and does passive network monitoring where does not have any noticeable impact on the ongoing connection. To use {SSHkex}, zero modiﬁcation needs to be done to the server. Thus, it is suitable for intrusion detection systems and high-interaction honeypot where the server shall not be modiﬁed.},
	pages = {301337},
	journaltitle = {Forensic Science International: Digital Investigation},
	shortjournal = {Forensic Science International: Digital Investigation},
	author = {Sentanoe, Stewart and Reiser, Hans P.},
	urldate = {2023-08-17},
	date = {2022-04},
	langid = {english},
	file = {Sentanoe et Reiser - 2022 - SSHkex Leveraging virtual machine introspection f.pdf:/home/clement/Zotero/storage/GBJCY9P8/Sentanoe et Reiser - 2022 - SSHkex Leveraging virtual machine introspection f.pdf:application/pdf},
}

@misc{fellicious_smartkex_2022,
	title = {{SmartKex}: Machine Learning Assisted {SSH} Keys Extraction From The Heap Dump},
	url = {http://arxiv.org/abs/2209.05243},
	shorttitle = {{SmartKex}},
	abstract = {Digital forensics is the process of extracting, preserving, and documenting evidence in digital devices. A commonly used method in digital forensics is to extract data from the main memory of a digital device. However, the main challenge is identifying the important data to be extracted. Several pieces of crucial information reside in the main memory, like usernames, passwords, and cryptographic keys such as {SSH} session keys. In this paper, we propose {SmartKex}, a machine-learning assisted method to extract session keys from heap memory snapshots of an {OpenSSH} process. In addition, we release an openly available dataset and the corresponding toolchain for creating additional data. Finally, we compare {SmartKex} with naive brute-force methods and empirically show that {SmartKex} can extract the session keys with high accuracy and high throughput. With the provided resources, we intend to strengthen the research on the intersection between digital forensics, cybersecurity, and machine learning.},
	number = {{arXiv}:2209.05243},
	publisher = {{arXiv}},
	author = {Fellicious, Christofer and Sentanoe, Stewart and Granitzer, Michael and Reiser, Hans P.},
	urldate = {2023-08-17},
	date = {2022-09-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.05243 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Fellicious et al. - 2022 - SmartKex Machine Learning Assisted SSH Keys Extra.pdf:/home/clement/Zotero/storage/FVESHJZY/Fellicious et al. - 2022 - SmartKex Machine Learning Assisted SSH Keys Extra.pdf:application/pdf},
}

@misc{huang_character-level_2016,
	title = {Character-level Convolutional Network for Text Classification Applied to Chinese Corpus},
	url = {http://arxiv.org/abs/1611.04358},
	abstract = {This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem.},
	number = {{arXiv}:1611.04358},
	publisher = {{arXiv}},
	author = {Huang, Weijie and Wang, Jun},
	urldate = {2023-08-17},
	date = {2016-11-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.04358 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Huang et Wang - 2016 - Character-level Convolutional Network for Text Cla.pdf:/home/clement/Zotero/storage/SCGHI52Z/Huang et Wang - 2016 - Character-level Convolutional Network for Text Cla.pdf:application/pdf},
}

@online{gite_how_2008,
	title = {How To Reuse {SSH} Connection To Speed Up Remote Login Process Using Multiplexing},
	url = {https://www.cyberciti.biz/faq/linux-unix-reuse-openssh-connection/},
	abstract = {Explains how to reuse existing {OpenSSH} connections using multiplexing to speed up an ssh login connection procedure on Linux/Unix/{macOS}/*{BSD}.},
	titleaddon = {{nixCraft}},
	author = {Gite, Vivek},
	urldate = {2022-10-21},
	date = {2008-08-20},
	langid = {american},
	file = {Snapshot:/home/clement/Zotero/storage/J2TCZAJP/linux-unix-reuse-openssh-connection.html:text/html},
}

@article{macqueen_methods_1967,
	title = {{SOME} {METHODS} {FOR} {CLASSIFICATION} {AND} {ANALYSIS} {OF} {MULTIVARIATE} {OBSERVATIONS}},
	volume = {{VOL}. 5.1},
	journaltitle = {{MULTIVARIATE} {OBSERVATIONS}},
	shortjournal = {Berkeley Symp. on Math. Statist. and Prob},
	author = {Macqueen, J},
	date = {1967},
	langid = {english},
	file = {Macqueen - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MU.pdf:/home/clement/Zotero/storage/459XE8DU/Macqueen - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MU.pdf:application/pdf},
}

@article{ester_density-based_1996,
	title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
	abstract = {Clustering algorithms are attractive for the task of class identiﬁcation in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efﬁciency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm {DBSCAN} relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. {DBSCAN} requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efﬁciency of {DBSCAN} using synthetic data and real data of the {SEQUOIA} 2000 benchmark. The results of our experiments demonstrate that (1) {DBSCAN} is signiﬁcantly more effective in discovering clusters of arbitrary shape than the well-known algorithm {CLARANS}, and that (2) {DBSCAN} outperforms {CLARANS} by a factor of more than 100 in terms of efﬁciency.},
	journaltitle = {{KDD}-96},
	author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jörg and Xu, Xiaowei},
	date = {1996-08-02},
	langid = {english},
	file = {Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf:/home/clement/Zotero/storage/M36EAE5Y/Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf:application/pdf},
}

@misc{von_luxburg_tutorial_2007,
	title = {A Tutorial on Spectral Clustering},
	url = {http://arxiv.org/abs/0711.0189},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved eﬃciently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the ﬁrst glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe diﬀerent graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several diﬀerent approaches. Advantages and disadvantages of the diﬀerent spectral clustering algorithms are discussed.},
	number = {{arXiv}:0711.0189},
	publisher = {{arXiv}},
	author = {von Luxburg, Ulrike},
	urldate = {2023-09-05},
	date = {2007-11-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {0711.0189 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:/home/clement/Zotero/storage/5WH2FANC/von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:application/pdf},
}

@misc{fellicious_machine_2022,
	title = {Machine Learning Assisted {SSH} Keys Extraction From The Heap Dump},
	rights = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/6537904},
	doi = {10.5281/ZENODO.6537904},
	abstract = {This dataset contains heap dump of {OpenSSH} that contains session keys. On the performance test data, we also include the {PCAP} file that contains the encrypted {SSH} network traffic. With the correct session keys, it can be decrypted.},
	version = {0.1},
	publisher = {Zenodo},
	author = {Fellicious, Christofer and Sentanoe, Stewart and Granitzer, Michael and Reiser, Hans P.},
	urldate = {2023-09-06},
	date = {2022-08-15},
	langid = {english},
	keywords = {memory heap dump, openssh, session keys},
}

@article{hogan_knowledge_2022,
	title = {Knowledge Graphs (Extended)},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2003.02320},
	doi = {10.1145/3447772},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	pages = {1--37},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	urldate = {2023-09-08},
	date = {2022-05-31},
	eprinttype = {arxiv},
	eprint = {2003.02320 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Databases},
}

@incollection{gomez-perez_understanding_2020,
	location = {Cham},
	title = {Understanding Word Embeddings and Language Models},
	isbn = {978-3-030-44830-1},
	url = {https://doi.org/10.1007/978-3-030-44830-1_3},
	abstract = {Early word embeddings algorithms like word2vec and {GloVe} generate static distributional representations for words regardless of the context and the sense in which the word is used in a given sentence, offering poor modeling of ambiguous words and lacking coverage for out-of-vocabulary words. Hence a new wave of algorithms based on training language models such as Open {AI} {GPT} and {BERT} has been proposed to generate contextual word embeddings that use as input word constituents allowing them to generate representations for out-of-vocabulary words by combining the word pieces. Recently, fine-tuning pre-trained language models that have been trained on large corpora have constantly advanced the state of the art for many {NLP} tasks.},
	pages = {17--31},
	booktitle = {A Practical Guide to Hybrid Natural Language Processing: Combining Neural Models and Knowledge Graphs for {NLP}},
	publisher = {Springer International Publishing},
	author = {Gomez-Perez, Jose Manuel and Denaux, Ronald and Garcia-Silva, Andres},
	editor = {Gomez-Perez, Jose Manuel and Denaux, Ronald and Garcia-Silva, Andres},
	urldate = {2023-09-08},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-44830-1_3},
}

@book{boddy_statistical_2009,
	title = {Statistical methods in practice: for scientists and technologists},
	publisher = {John Wiley \& Sons},
	author = {Boddy, Richard and Smith, Gordon},
	date = {2009},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
}

@article{jordan_machine_2015,
	title = {Machine learning: Trends, perspectives, and prospects},
	volume = {349},
	url = {https://www.science.org/doi/full/10.1126/science.aaa8415},
	pages = {255--260},
	number = {6245},
	journaltitle = {Science},
	author = {Jordan, Michael I and Mitchell, Tom M},
	date = {2015},
	note = {Publisher: American Association for the Advancement of Science},
}

@article{zhou_evaluating_2021,
	title = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
	volume = {10},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/5/593},
	doi = {10.3390/electronics10050593},
	shorttitle = {Evaluating the Quality of Machine Learning Explanations},
	abstract = {The most successful Machine Learning ({ML}) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of {ML} explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of {ML} explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of {ML} explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods.},
	pages = {593},
	number = {5},
	journaltitle = {Electronics},
	shortjournal = {Electronics},
	author = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
	urldate = {2023-09-11},
	date = {2021-03-04},
	langid = {english},
	file = {Full Text:/home/clement/Zotero/storage/WE58DR6J/Zhou et al. - 2021 - Evaluating the Quality of Machine Learning Explana.pdf:application/pdf},
}

@incollection{farin_graphviz_2004,
	location = {Berlin, Heidelberg},
	title = {Graphviz and Dynagraph — Static and Dynamic Graph Drawing Tools},
	isbn = {978-3-642-62214-4 978-3-642-18638-7},
	url = {http://link.springer.com/10.1007/978-3-642-18638-7_6},
	pages = {127--148},
	booktitle = {Graph Drawing Software},
	publisher = {Springer Berlin Heidelberg},
	author = {Ellson, John and Gansner, Emden R. and Koutsofios, Eleftherios and North, Stephen C. and Woodhull, Gordon},
	editor = {Jünger, Michael and Mutzel, Petra},
	editorb = {Farin, Gerald and Hege, Hans-Christian and Hoffman, David and Johnson, Christopher R. and Polthier, Konrad},
	editorbtype = {redactor},
	urldate = {2023-09-11},
	date = {2004},
	langid = {english},
	doi = {10.1007/978-3-642-18638-7_6},
	note = {Series Title: Mathematics and Visualization},
	file = {Ellson et al. - 2004 - Graphviz and Dynagraph — Static and Dynamic Graph .pdf:/home/clement/Zotero/storage/RVIN399N/Ellson et al. - 2004 - Graphviz and Dynagraph — Static and Dynamic Graph .pdf:application/pdf},
}

@inproceedings{wang_structural_2016,
	location = {San Francisco California {USA}},
	title = {Structural Deep Network Embedding},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939753},
	doi = {10.1145/2939672.2939753},
	eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {1225--1234},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Wang, Daixin and Cui, Peng and Zhu, Wenwu},
	urldate = {2023-09-11},
	date = {2016-08-13},
	langid = {english},
}

@book{goodfellow_deep_2016,
	title = {Deep learning},
	url = {https://books.google.de/books?hl=en&lr=&id=omivDQAAQBAJ&oi=fnd&pg=PR5&dq=deep+learning&ots=MNV2eosBRS&sig=jN2QwFikq3g_YqU3hJVPEP0XIJ4&redir_esc=y#v=onepage&q=deep%20learning&f=false},
	publisher = {{MIT} press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
}

@inproceedings{nguyen_novel_2018,
	title = {A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network},
	url = {https://doi.org/10.18653%2Fv1%2Fn18-2053},
	doi = {10.18653/v1/n18-2053},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Dai Quoc and Nguyen, Tu Dinh and Nguyen, Dat Quoc and Phung, Dinh},
	date = {2018},
}

@inproceedings{schlichtkrull_modeling_2018,
	title = {Modeling relational data with graph convolutional networks},
	url = {https://arxiv.org/pdf/1703.06103.pdf},
	pages = {593--607},
	booktitle = {The Semantic Web: 15th International Conference, {ESWC} 2018, Heraklion, Crete, Greece, June 3–7, 2018, Proceedings 15},
	publisher = {Springer},
	author = {Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
	date = {2018},
}

@article{degraeve_r-gcn_2022,
	title = {R-{GCN}: the R could stand for random},
	url = {https://arxiv.org/pdf/2203.02424.pdf},
	journaltitle = {{arXiv}:2203.02424 preprint},
	author = {Degraeve, Vic and Vandewiele, Gilles and Ongenae, Femke and Van Hoecke, Sofie},
	date = {2022},
}

@article{yao_kg-bert_2019,
	title = {{KG}-{BERT}: {BERT} for knowledge graph completion},
	url = {https://arxiv.org/pdf/1909.03193.pdf},
	journaltitle = {{arXiv} preprint {arXiv}:1909.03193},
	author = {Yao, Liang and Mao, Chengsheng and Luo, Yuan},
	date = {2019},
}

@inproceedings{liu_kg-bart_2021,
	title = {Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning},
	volume = {35},
	url = {file:///home/onyr/Downloads/16796-Article%20Text-20290-1-2-20210518.pdf},
	pages = {6418--6425},
	booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Liu, Ye and Wan, Yao and He, Lifang and Peng, Hao and Philip, S Yu},
	date = {2021},
	note = {Issue: 7},
}

@article{ankerst_optics_1999,
	title = {{OPTICS}: Ordering Points To Identify the Clustering Structure},
	volume = {28},
	doi = {10.1145/304181.304187},
	abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only ‘traditional’ clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
	pages = {49--60},
	journaltitle = {{ACM} {SIGMOD} Record},
	author = {Ankerst, Mihael and Breunig, Markus M and Kriegel, Hans-Peter and Sander, Jörg},
	date = {1999-06},
	langid = {english},
	file = {Ankerst et al. - OPTICS Ordering Points To Identify the Clustering.pdf:/home/clement/Zotero/storage/8ABIFAU9/Ankerst et al. - OPTICS Ordering Points To Identify the Clustering.pdf:application/pdf},
}

@online{gloger_malloc_2001,
	title = {Malloc implementation for multiple threads without lock contention},
	rights = {Copyright (C) 1996-2018 Free Software Foundation, Inc.},
	url = {https://elixir.bootlin.com/glibc/glibc-2.28/source/malloc/malloc.c},
	author = {Gloger, Wolfram and Lea, Doug},
	urldate = {2023-09-22},
	date = {2001},
	note = {Publisher: Free Software Foundation, Inc.},
	keywords = {{GNU} C Library, ptmalloc2},
}

@online{delorie_malloc_2023,
	title = {Malloc Internals},
	url = {https://sourceware.org/glibc/wiki/MallocInternals},
	author = {Delorie, D. J. and Weimer, Florian and O'Donell, Carlos and Schwab, Andreas},
	urldate = {2023-09-25},
	date = {2023},
	note = {Publisher: Sourceware},
	keywords = {{GNU} C Library, Malloc Internals},
}

@online{unknown_how_2023,
	title = {How does glibc malloc work?},
	url = {https://reverseengineering.stackexchange.com/questions/15033/how-does-glibc-malloc-work/15038#15038},
	author = {{Unknown}},
	date = {2023},
}
