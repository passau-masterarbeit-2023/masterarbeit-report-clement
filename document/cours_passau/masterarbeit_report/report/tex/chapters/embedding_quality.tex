\chapter{Embedding quality}\label{chap:embedding_quality}

\paragraph{}The quality of embeddings is paramount in machine learning, particularly when the objective is to identify specific \glspl{structure} within data, such as the ones holding SSH keys. It becomes essential to juxtapose the performances of all embeddings in this context. An optimal embedding should proficiently discern the \glspl{structure} containing SSH keys across the entire spectrum of openSSH use cases and for every conceivable key size. This necessitates the utilization of the complete dataset, with the training subset dedicated to model training and the validation subset for testing. Addressing this from a machine learning classification perspective, the random forest model, as elucidated in \ref{seq:background:some_ml_common_models}, emerges as the classifier of choice.

\section{Feature Selection and Dataset Challenges}

\paragraph{}In the quest for fairness across various embeddings and to circumvent the curse of dimensionality, it's imperative to maintain a uniform feature count across all embeddings. This is where feature engineering shines. The Pearson correlation method, elaborated in \ref{seq:background:correlation_tests}, is harnessed to meticulously select the 8 most salient features for each embedding. This count is a judicious compromise, ensuring the features are both succinct in number and information-rich. However, the dataset presents its own set of challenges. The instances of \glspl{structure} containing SSH keys are dwarfed by those devoid of them, leading to a pronounced dataset imbalance. To counteract this skewness, the \gls{smote} technique, as referenced in \ref{seq:background:imbalanced_data}, is employed.

\section{Implementation and Evaluation Metrics}

\paragraph{}The implementation leans heavily on the scikit-learn library \cite{pedregosa_scikit-learn_2011} in Python, which provides the tools for the random forest classifier, Pearson correlation, and the \gls{smote} algorithm. Concurrently, the pandas library is indispensable for the efficient loading and manipulation of the dataset. Before diving into the analysis, it's crucial to ensure the embedding's integrity. This involves a rigorous sanity check, especially given the potential for corruption, such as NaN values, which are incompatible with the \gls{smote} technique. To guarantee the reproducibility of results, a consistent random seed is employed for both the random forest classifier and the \gls{smote} algorithm. For a comprehensive evaluation, the Pearson correlation matrix is preserved for each embedding. Moreover, a suite of metrics, including precision, recall, f1-score, AUC, and the confusion matrix (encompassing true positives, true negatives, false positives, and false negatives), is meticulously saved for every embedding.
