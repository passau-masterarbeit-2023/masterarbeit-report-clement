
@online{gite_how_2008,
	title = {How To Reuse {SSH} Connection To Speed Up Remote Login Process Using Multiplexing},
	url = {https://www.cyberciti.biz/faq/linux-unix-reuse-openssh-connection/},
	abstract = {Explains how to reuse existing {OpenSSH} connections using multiplexing to speed up an ssh login connection procedure on Linux/Unix/{macOS}/*{BSD}.},
	titleaddon = {{nixCraft}},
	author = {Gite, Vivek},
	urldate = {2022-10-21},
	date = {2008-08-20},
	langid = {american},
	file = {Snapshot:/home/clement/Zotero/storage/J2TCZAJP/linux-unix-reuse-openssh-connection.html:text/html},
}

@misc{huang_character-level_2016,
	title = {Character-level Convolutional Network for Text Classification Applied to Chinese Corpus},
	url = {http://arxiv.org/abs/1611.04358},
	abstract = {This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem.},
	number = {{arXiv}:1611.04358},
	publisher = {{arXiv}},
	author = {Huang, Weijie and Wang, Jun},
	urldate = {2023-08-17},
	date = {2016-11-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.04358 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Huang et Wang - 2016 - Character-level Convolutional Network for Text Cla.pdf:/home/clement/Zotero/storage/SCGHI52Z/Huang et Wang - 2016 - Character-level Convolutional Network for Text Cla.pdf:application/pdf},
}

@misc{fellicious_smartkex_2022,
	title = {{SmartKex}: Machine Learning Assisted {SSH} Keys Extraction From The Heap Dump},
	url = {http://arxiv.org/abs/2209.05243},
	shorttitle = {{SmartKex}},
	abstract = {Digital forensics is the process of extracting, preserving, and documenting evidence in digital devices. A commonly used method in digital forensics is to extract data from the main memory of a digital device. However, the main challenge is identifying the important data to be extracted. Several pieces of crucial information reside in the main memory, like usernames, passwords, and cryptographic keys such as {SSH} session keys. In this paper, we propose {SmartKex}, a machine-learning assisted method to extract session keys from heap memory snapshots of an {OpenSSH} process. In addition, we release an openly available dataset and the corresponding toolchain for creating additional data. Finally, we compare {SmartKex} with naive brute-force methods and empirically show that {SmartKex} can extract the session keys with high accuracy and high throughput. With the provided resources, we intend to strengthen the research on the intersection between digital forensics, cybersecurity, and machine learning.},
	number = {{arXiv}:2209.05243},
	publisher = {{arXiv}},
	author = {Fellicious, Christofer and Sentanoe, Stewart and Granitzer, Michael and Reiser, Hans P.},
	urldate = {2023-08-17},
	date = {2022-09-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.05243 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Fellicious et al. - 2022 - SmartKex Machine Learning Assisted SSH Keys Extra.pdf:/home/clement/Zotero/storage/FVESHJZY/Fellicious et al. - 2022 - SmartKex Machine Learning Assisted SSH Keys Extra.pdf:application/pdf},
}

@article{sentanoe_sshkex_2022,
	title = {{SSHkex}: Leveraging virtual machine introspection for extracting {SSH} keys and decrypting {SSH} network traffic},
	volume = {40},
	issn = {26662817},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666281722000063},
	doi = {10.1016/j.fsidi.2022.301337},
	shorttitle = {{SSHkex}},
	abstract = {Nowadays, many users are using an encrypted channel to communicate with a remote resource server. Such a channel provides a high degree of privacy and conﬁdentiality. Secure Shell ({SSH}) is one of the most commonly used methods to connect to a server remotely. {SSH} provides privacy and conﬁdentiality by encrypting network trafﬁc between the client and the server. The encryption makes the learning process of malicious activities over {SSH} is challenging, especially by just analyzing the network trafﬁc. To overcome the problem, we can leverage Virtual machine introspection ({VMI}). {VMI} allows direct memory access of a virtual machine ({VM}) including accessing data of an {SSH} process. However, the current prototype suffers from high overhead since it extracts every single plain text {SSH} network payload from memory and the extraction process requires the virtual machine ({VM}) to be momentarily paused. In this paper, we introduce {SSHkex}, a tool that also leverages {VMI} to extracts {SSH}'s session keys from a server's memory. Our approach only needs to pause the {VM} twice to extract the session keys for each {SSH} session and does passive network monitoring where does not have any noticeable impact on the ongoing connection. To use {SSHkex}, zero modiﬁcation needs to be done to the server. Thus, it is suitable for intrusion detection systems and high-interaction honeypot where the server shall not be modiﬁed.},
	pages = {301337},
	journaltitle = {Forensic Science International: Digital Investigation},
	shortjournal = {Forensic Science International: Digital Investigation},
	author = {Sentanoe, Stewart and Reiser, Hans P.},
	urldate = {2023-08-17},
	date = {2022-04},
	langid = {english},
	file = {Sentanoe et Reiser - 2022 - SSHkex Leveraging virtual machine introspection f.pdf:/home/clement/Zotero/storage/GBJCY9P8/Sentanoe et Reiser - 2022 - SSHkex Leveraging virtual machine introspection f.pdf:application/pdf},
}

@article{hiester_file_2018,
	title = {File Fragment Classification Using Neural Networks with Lossless Representations},
	abstract = {This study explores the use of neural networks as universal models for classifying ﬁle fragments. This approach diﬀers from previous work in its lossless feature representation, with fragments’ bits as direct input, and its use of feedforward, recurrent, and convolutional networks as classiﬁers, whereas previous work has only tested feedforward networks. Due to the study’s exploratory nature, the models were not directly evaluated in a practical setting; rather, easily reproducible experiments were performed to attempt to answer the intial question of whether this approach is worthwhile to pursue further, especially due to its high computational cost. The experiments tested classiﬁcation of fragments of homogeneous ﬁle types as an idealized case, rather than using a realistic set of types, because the types of interest are highly application-dependent. The recurrent networks achieved 98 percent accuracy in distinguishing 4 ﬁle types, suggesting that this approach may be capable of yielding models with sufﬁcient performance for practical applications. The potential applications depend mainly on the model performance gains achievable by future work but include binary mapping, deep packet inspection, and ﬁle carving.},
	journaltitle = {East Tennessee State University},
	author = {Hiester, Luke},
	urldate = {2023-08-21},
	date = {2018-05},
	langid = {english},
	file = {Hiester - File Fragment Classification Using Neural Networks.pdf:/home/clement/Zotero/storage/J5IUYN87/Hiester - File Fragment Classification Using Neural Networks.pdf:application/pdf},
}

@article{lai_recurrent_2015,
	title = {Recurrent Convolutional Neural Networks for Text Classification},
	volume = {29},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9513},
	doi = {10.1609/aaai.v29i1.9513},
	abstract = {Text classiﬁcation is a foundational task in many {NLP} applications. Traditional text classiﬁers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classiﬁcation without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classiﬁcation to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	urldate = {2023-08-23},
	date = {2015-02-19},
	langid = {english},
	file = {Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text C.pdf:/home/clement/Zotero/storage/CP2DU5RB/Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text C.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	volume = {30},
	pages = {5998--6008},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-08-23},
	date = {2017},
	file = {Attention Is All You Need.pdf:/home/clement/Zotero/storage/QEQIBNKM/Attention Is All You Need.pdf:application/pdf},
}

@misc{bai_empirical_2018,
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as {LSTMs} across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/{TCN}.},
	number = {{arXiv}:1803.01271},
	publisher = {{arXiv}},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	urldate = {2023-08-23},
	date = {2018-04-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.01271 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:/home/clement/Zotero/storage/TM3UYTWZ/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2023-08-23},
	date = {1997},
	note = {Publisher: {MIT} Press},
	file = {Long short-term memory.pdf:/home/clement/Zotero/storage/U2UBGXGB/Long short-term memory.pdf:application/pdf},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to Sequence Learning with Neural Networks},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}’14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}’s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difﬁculty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	number = {{arXiv}:1409.3215},
	publisher = {{arXiv}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	urldate = {2023-08-23},
	date = {2014-12-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.3215 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:/home/clement/Zotero/storage/VR9CW4R4/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@misc{chung_empirical_2014,
	title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks ({RNNs}). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory ({LSTM}) unit and a recently proposed gated recurrent unit ({GRU}). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found {GRU} to be comparable to {LSTM}.},
	number = {{arXiv}:1412.3555},
	publisher = {{arXiv}},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, {KyungHyun} and Bengio, Yoshua},
	urldate = {2023-08-23},
	date = {2014-12-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.3555 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:/home/clement/Zotero/storage/KI4CBV2F/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf},
}

@article{shannon_mathematical_1948,
	title = {A Mathematical Theory of Communication},
	volume = {27},
	pages = {379--423},
	journaltitle = {The Bell System Technical Journa},
	author = {Shannon, C E},
	date = {1948-10},
	langid = {english},
	file = {Shannon - A Mathematical Theory of Communication.pdf:/home/clement/Zotero/storage/39BJAWAH/Shannon - A Mathematical Theory of Communication.pdf:application/pdf},
}
