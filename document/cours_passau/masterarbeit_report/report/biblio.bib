
@inproceedings{laaksonen_classification_1996,
	location = {Washington, {DC}, {USA}},
	title = {Classification with learning k-nearest neighbors},
	volume = {3},
	isbn = {978-0-7803-3210-2},
	url = {http://ieeexplore.ieee.org/document/549118/},
	doi = {10.1109/ICNN.1996.549118},
	abstract = {The nearest neighbor ({NN}) classifiers, especially the k-{NN} algorithm, are among the simplest and yet most efficient classification rules and are widely used in practice. We introduce three adaptation rules that can be used in iterative training of a k-{NN} classifier. This is a novel approach both from the statistical pattern recognition and the supervised neural network learning points of view. The suggested learning rules resemble those of the well-known Learning Vector Quantization ({LVQ}) method, but at the same time the classifier utilizes the fact that increasing the number of samples that the classification is based on leads to improved classification accuracy. The performances of the suggested learning rules are compared with the usual k-{NN} rules and the {LVQl} algorithm.},
	eventtitle = {International Conference on Neural Networks ({ICNN}'96)},
	pages = {1480--1483},
	booktitle = {Proceedings of International Conference on Neural Networks ({ICNN}'96)},
	publisher = {{IEEE}},
	author = {Laaksonen, J. and Oja, E.},
	urldate = {2023-08-30},
	date = {1996},
	langid = {english},
	file = {Laaksonen et Oja - 1996 - Classification with learning k-nearest neighbors.pdf:/home/clement/Zotero/storage/VKKSVUW6/Laaksonen et Oja - 1996 - Classification with learning k-nearest neighbors.pdf:application/pdf},
}

@article{wu_analysis_2006,
	title = {Analysis of Support Vector Machine Classiﬁcation},
	volume = {8},
	abstract = {This paper studies support vector machine classiﬁcation algorithms. We analyze the 1-norm soft margin classiﬁer. The consistency is considered in two forms. When the regularization error decays to zero, the Bayes-risk consistency is proved and learning rates are derived by means of techniques of uniform convergence. The main diﬃculty we overcome here is to bound the oﬀset. For the consistency with hypothesis space, we present a counterexample.},
	number = {2},
	journaltitle = {Journal of Computational Analysis \& Applications},
	author = {Wu, Qiang and Zhou, Ding-Xuan},
	date = {2006},
	langid = {english},
	file = {Wu et Zhou - Analysis of Support Vector Machine Classiﬁcation.pdf:/home/clement/Zotero/storage/3E5TTXSF/Wu et Zhou - Analysis of Support Vector Machine Classiﬁcation.pdf:application/pdf},
}

@article{probst_hyperparameters_2019,
	title = {Hyperparameters and Tuning Strategies for Random Forest},
	volume = {9},
	issn = {1942-4787, 1942-4795},
	url = {http://arxiv.org/abs/1804.03515},
	doi = {10.1002/widm.1301},
	abstract = {The random forest algorithm ({RF}) has several hyperparameters that have to be set by the user, e.g., the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain and the number of trees. In this paper, we ﬁrst provide a literature review on the parameters’ inﬂuence on the prediction performance and on variable importance measures.},
	pages = {e1301},
	number = {3},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	shortjournal = {{WIREs} Data Min \& Knowl},
	author = {Probst, Philipp and Wright, Marvin and Boulesteix, Anne-Laure},
	urldate = {2023-08-30},
	date = {2019-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.03515 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Probst et al. - 2019 - Hyperparameters and Tuning Strategies for Random F.pdf:/home/clement/Zotero/storage/ATYXZF23/Probst et al. - 2019 - Hyperparameters and Tuning Strategies for Random F.pdf:application/pdf},
}

@article{kotsiantis_decision_2013,
	title = {Decision trees: a recent overview},
	volume = {39},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-011-9272-4},
	doi = {10.1007/s10462-011-9272-4},
	shorttitle = {Decision trees},
	abstract = {Decision tree techniques have been widely used to build classiﬁcation models as such models closely resemble human reasoning and are easy to understand. This paper describes basic decision tree issues and current research points. Of course, a single article cannot be a complete review of all algorithms (also known induction classiﬁcation trees), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
	pages = {261--283},
	number = {4},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Kotsiantis, S. B.},
	urldate = {2023-08-30},
	date = {2013-04},
	langid = {english},
	file = {Kotsiantis - 2013 - Decision trees a recent overview.pdf:/home/clement/Zotero/storage/94EWYUZR/Kotsiantis - 2013 - Decision trees a recent overview.pdf:application/pdf},
}

@collection{ambrosius_topics_2007,
	location = {Totowa, N.J},
	title = {Topics in biostatistics},
	isbn = {978-1-58829-531-6},
	series = {Methods in molecular biology},
	abstract = {Presents a multidisciplinary survey of biostatics methods, each illustrated with hands-on examples. Methods range from the elementary, including descriptive statistics, study design, statistical interference, categorical variables, evaluation of diagnostic tests, comparison of means, linear regression, and logistic regression. These introductory methods create a portfolio of biostatistical techniques for both novice and expert researchers. More complicated statistical methods are introduced as well, including those requiring either collaboration with a biostatistician or the use of a statistical package. Specific topics of interest include microarray analysis, missing data techniques, power and sample size, statistical methods in genetics.--},
	pagetotal = {528},
	number = {404},
	publisher = {Humana Press},
	editor = {Ambrosius, Walter T.},
	date = {2007},
	langid = {english},
	note = {{OCLC}: ocn159977868},
	keywords = {Aufsatzsammlung, Biometry, Biostatistik, Informatics, Methodology, methods},
	file = {Ambrosius - 2007 - Topics in biostatistics.pdf:/home/clement/Zotero/storage/T7A2ISBW/Ambrosius - 2007 - Topics in biostatistics.pdf:application/pdf},
}

@article{nick_logistic_2007,
	title = {Logistic regression},
	pages = {273--301},
	journaltitle = {Topics in biostatistics},
	author = {Nick, Todd G and Campbell, Kathleen M},
	date = {2007},
	note = {Publisher: Springer},
	file = {Nick et Campbell - 2007 - Logistic regression.pdf:/home/clement/Zotero/storage/NS5FEUBX/Nick et Campbell - 2007 - Logistic regression.pdf:application/pdf},
}

@article{ramyachitra_imbalanced_2014,
	title = {{IMBALANCED} {DATASET} {CLASSIFICATION} {AND} {SOLUTIONS}: A {REVIEW}},
	volume = {5},
	abstract = {Imbalanced data set problem occurs in classification, where the number of instances of one class is much lower than the instances of the other classes. The main challenge in imbalance problem is that the small classes are often more useful, but standard classifiers tend to be weighed down by the huge classes and ignore the tiny ones. In machine learning the imbalanced datasets has become a critical problem and also usually found in many applications such as detection of fraudulent calls, bio-medical, engineering, remote-sensing, computer society and manufacturing industries. In order to overcome the problems several approaches have been proposed. In this paper a study on Imbalanced dataset problem and the solution is given.},
	number = {4},
	journaltitle = {International Journal of Computing and Business Research},
	author = {Ramyachitra, Dr D and Manikandan, P},
	date = {2014},
	langid = {english},
	file = {Ramyachitra et Manikandan - 2014 - IMBALANCED DATASET CLASSIFICATION AND SOLUTIONS A.pdf:/home/clement/Zotero/storage/7GAZ68WX/Ramyachitra et Manikandan - 2014 - IMBALANCED DATASET CLASSIFICATION AND SOLUTIONS A.pdf:application/pdf},
}

@online{noauthor_certcc_nodate,
	title = {{CERT}/{CC} Vulnerability Note {VU}\#13877},
	url = {https://www.kb.cert.org},
	abstract = {Weak {CRC} allows packet injection into {SSH} sessions encrypted with block ciphers},
	urldate = {2023-08-30},
}

@inproceedings{yurcik_first_2005,
	title = {A first step toward detecting {SSH} identity theft in {HPC} cluster environments: discriminating masqueraders based on command behavior},
	volume = {1},
	doi = {10.1109/CCGRID.2005.1558542},
	shorttitle = {A first step toward detecting {SSH} identity theft in {HPC} cluster environments},
	abstract = {Recent attacks enabled by stolen authentication passwords and keys have allowed intruders to masquerade as legitimate users on high performance computing clusters. With the motivation of detecting masqueraders on clusters, this work seeks to discriminate different types of users based on their command behavior - in particular, user command behavior on a multi-user public machine versus user command behavior on a high performance computing cluster. Our intuition is that these users act differently and the unique high performance cluster environment is constrained such that command behavior discrimination is enhanced versus enterprise environments. We formalize this into a classification problem to be solved by a support vector machine with {TF}-{IDF} feature construction techniques from the field of Information Retrieval. We present results showing the effectiveness of this approach exhibiting high precision depending on the length of monitoring in both time and number of commands. In particular we show that as few as 10 commands may be enough to recognize a masquerading attacker on a high performance computing cluster.},
	eventtitle = {{CCGrid} 2005. {IEEE} International Symposium on Cluster Computing and the Grid, 2005.},
	pages = {111--120 Vol. 1},
	booktitle = {{CCGrid} 2005. {IEEE} International Symposium on Cluster Computing and the Grid, 2005.},
	author = {Yurcik, W. and Liu, Chao},
	date = {2005-05},
	keywords = {Authentication, Chaos, High performance computing, Information retrieval, Inspection, Monitoring, Performance analysis, Security, Support vector machine classification, Support vector machines},
}

@article{koppen_curse_2000,
	title = {The curse of dimensionality},
	volume = {1},
	abstract = {In this text, some question related to higher dimensional geometrical spaces will be discussed. The goal is to give the reader a feeling for geometric distortions related to the use of such spaces (e.g. as search spaces).},
	pages = {4--8},
	author = {Koppen, Mario},
	date = {2000},
	langid = {english},
	file = {Koppen - The curse of dimensionality.pdf:/home/clement/Zotero/storage/983R36QZ/Koppen - The curse of dimensionality.pdf:application/pdf},
}

@incollection{hutchison_curse_2005,
	location = {Berlin, Heidelberg},
	title = {The Curse of Dimensionality in Data Mining and Time Series Prediction},
	volume = {3512},
	isbn = {978-3-540-26208-4 978-3-540-32106-4},
	url = {http://link.springer.com/10.1007/11494669_93},
	pages = {758--770},
	booktitle = {Computational Intelligence and Bioinspired Systems},
	publisher = {Springer Berlin Heidelberg},
	author = {Verleysen, Michel and François, Damien},
	editor = {Cabestany, Joan and Prieto, Alberto and Sandoval, Francisco},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2023-08-30},
	date = {2005},
	doi = {10.1007/11494669_93},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{khalid_survey_2014,
	title = {A survey of feature selection and feature extraction techniques in machine learning},
	doi = {10.1109/SAI.2014.6918213},
	abstract = {Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.},
	eventtitle = {2014 Science and Information Conference},
	pages = {372--378},
	booktitle = {2014 Science and Information Conference},
	author = {Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
	date = {2014-08},
	keywords = {Accuracy, Age Related Macula Degeneration ({AMD}), Algorithm design and analysis, Correlation, Correlation Based Method, Feature extraction, Feature Extraction/Transformation, Feature Selection, Feature Subset Selection, {FSA}'s, {ICA}, Noise, {PCA}, Principal component analysis, Redundancy, {RELIEF}},
	file = {IEEE Xplore Abstract Record:/home/clement/Zotero/storage/9ILHCP4W/6918213.html:text/html;IEEE Xplore Full Text PDF:/home/clement/Zotero/storage/M34E8V5U/Khalid et al. - 2014 - A survey of feature selection and feature extracti.pdf:application/pdf},
}

@misc{khurana_feature_2017,
	title = {Feature Engineering for Predictive Modeling using Reinforcement Learning},
	url = {http://arxiv.org/abs/1709.07150},
	abstract = {Feature engineering is a crucial step in the process of predictive modeling. It involves the transformation of given feature space, typically using mathematical functions, with the objective of reducing the modeling error for a given target. However, there is no well-deﬁned basis for performing effective feature engineering. It involves domain knowledge, intuition, and most of all, a lengthy process of trial and error. The human attention involved in overseeing this process signiﬁcantly inﬂuences the cost of model generation. We present a new framework to automate feature engineering. It is based on performance driven exploration of a transformation graph, which systematically and compactly enumerates the space of given options. A highly efﬁcient exploration strategy is derived through reinforcement learning on past examples.},
	number = {{arXiv}:1709.07150},
	publisher = {{arXiv}},
	author = {Khurana, Udayan and Samulowitz, Horst and Turaga, Deepak},
	urldate = {2023-08-30},
	date = {2017-09-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.07150 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Khurana et al. - 2017 - Feature Engineering for Predictive Modeling using .pdf:/home/clement/Zotero/storage/R5YW6N45/Khurana et al. - 2017 - Feature Engineering for Predictive Modeling using .pdf:application/pdf},
}

@article{martinez_garre_novel_2021,
	title = {A novel Machine Learning-based approach for the detection of {SSH} botnet infection},
	volume = {115},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X20303265},
	doi = {10.1016/j.future.2020.09.004},
	abstract = {Botnets are causing severe damages to users, companies, and governments through information theft, abuse of online services, {DDoS} attacks, etc. Although significant research is being made to detect them and mitigate their effect, they are exponentially increasing due to new zero-day attacks, a variation of their behavior, and obfuscation techniques. High Interaction Honeypots ({HIH}) are the only honeypots able to capture attacks and log all the information generated by attackers when setting up a botnet. The data generated is being processed using Machine Learning ({ML}) techniques for detection since they can detect hidden patterns. However, so far, research has been focused on intermediate phases of the botnet’s life cycle during operation, underestimating the initial phase of infection. To the best of our knowledge, this is the first solution in the infection phase of {SSH}-based botnets. Therefore, we have designed an approach based on an {SSH}-based {HIH} to generate a dataset consisting of executed commands and network information. Herein, we have applied {ML} techniques for the development of a real-time detection model. This approach reached a very high level of prediction and zero false negatives. Indeed, our system detected all known and unknown {SSH} sessions intended to infect our honeypots. Thus, our research has demonstrated that new {SSH} infections can be detected through {ML} techniques.},
	pages = {387--396},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Martínez Garre, José Tomás and Gil Pérez, Manuel and Ruiz-Martínez, Antonio},
	urldate = {2023-08-30},
	date = {2021-02-01},
	keywords = {Botnet, High interaction, Honeypot, Machine learning, Zero-day malware},
}

@article{hinton_reducing_2006,
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1127647},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	pages = {504--507},
	number = {5786},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	urldate = {2023-08-30},
	date = {2006-07-28},
	langid = {english},
	file = {Hinton et Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf:/home/clement/Zotero/storage/JLNIYY7J/Hinton et Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf:application/pdf},
}

@article{wheeler_problems_2011,
	title = {Problems with Skewness and Kurtosis},
	journaltitle = {Quality Digest Daily},
	author = {Wheeler, Donald J},
	date = {2011-08-01},
	langid = {english},
	file = {Wheeler - Problems with Skewness and Kurtosis.pdf:/home/clement/Zotero/storage/2WCWNHPG/Wheeler - Problems with Skewness and Kurtosis.pdf:application/pdf},
}

@article{cain_univariate_2017,
	title = {Univariate and multivariate skewness and kurtosis for measuring nonnormality: Prevalence, influence and estimation},
	volume = {49},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-016-0814-1},
	doi = {10.3758/s13428-016-0814-1},
	shorttitle = {Univariate and multivariate skewness and kurtosis for measuring nonnormality},
	abstract = {Nonnormality of univariate data has been extensively examined previously (Blanca et al., Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 9(2), 78–84, 2013; Miceeri, Psychological Bulletin, 105(1), 156, 1989). However, less is known of the potential nonnormality of multivariate data although multivariate analysis is commonly used in psychological and educational research. Using univariate and multivariate skewness and kurtosis as measures of nonnormality, this study examined 1,567 univariate distriubtions and 254 multivariate distributions collected from authors of articles published in Psychological Science and the American Education Research Journal. We found that 74 \% of univariate distributions and 68 \% multivariate distributions deviated from normal distributions. In a simulation study using typical values of skewness and kurtosis that we collected, we found that the resulting type I error rates were 17 \% in a t-test and 30 \% in a factor analysis under some conditions. Hence, we argue that it is time to routinely report skewness and kurtosis along with other summary statistics such as means and variances. To facilitate future report of skewness and kurtosis, we provide a tutorial on how to compute univariate and multivariate skewness and kurtosis by {SAS}, {SPSS}, R and a newly developed Web application.},
	pages = {1716--1735},
	number = {5},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Cain, Meghan K. and Zhang, Zhiyong and Yuan, Ke-Hai},
	urldate = {2023-08-30},
	date = {2017-10},
	langid = {english},
	file = {Cain et al. - 2017 - Univariate and multivariate skewness and kurtosis .pdf:/home/clement/Zotero/storage/MVY7S7S9/Cain et al. - 2017 - Univariate and multivariate skewness and kurtosis .pdf:application/pdf},
}

@article{gehring_convolutional_2017,
	title = {Convolutional Sequence to Sequence Learning},
	url = {https://arxiv.org/pdf/1705.03122.pdf},
	journaltitle = {Facebook {AI} Research},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
	date = {2017-07-25},
	langid = {english},
	file = {Gehring et al. - Convolutional Sequence to Sequence Learning.pdf:/home/clement/Zotero/storage/RWL9YDL3/Gehring et al. - Convolutional Sequence to Sequence Learning.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-Based Learning Applied to Document Recognition},
	journaltitle = {proc of the {IEEE}},
	author = {{LeCun}, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
	date = {1998},
	langid = {english},
	file = {LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:/home/clement/Zotero/storage/JTT9JKFY/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:application/pdf},
}

@article{shannon_mathematical_1948,
	title = {A Mathematical Theory of Communication},
	volume = {27},
	pages = {379--423},
	journaltitle = {The Bell System Technical Journa},
	author = {Shannon, C E},
	date = {1948-10},
	langid = {english},
	file = {Shannon - A Mathematical Theory of Communication.pdf:/home/clement/Zotero/storage/39BJAWAH/Shannon - A Mathematical Theory of Communication.pdf:application/pdf},
}

@misc{chung_empirical_2014,
	title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks ({RNNs}). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory ({LSTM}) unit and a recently proposed gated recurrent unit ({GRU}). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found {GRU} to be comparable to {LSTM}.},
	number = {{arXiv}:1412.3555},
	publisher = {{arXiv}},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, {KyungHyun} and Bengio, Yoshua},
	urldate = {2023-08-23},
	date = {2014-12-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.3555 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:/home/clement/Zotero/storage/KI4CBV2F/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf},
}

@misc{sutskever_sequence_2014,
	title = {Sequence to Sequence Learning with Neural Networks},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}’14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}’s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difﬁculty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	number = {{arXiv}:1409.3215},
	publisher = {{arXiv}},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	urldate = {2023-08-23},
	date = {2014-12-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.3215 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:/home/clement/Zotero/storage/VR9CW4R4/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2023-08-23},
	date = {1997},
	note = {Publisher: {MIT} Press},
	file = {Long short-term memory.pdf:/home/clement/Zotero/storage/U2UBGXGB/Long short-term memory.pdf:application/pdf},
}

@misc{bai_empirical_2018,
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as {LSTMs} across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/{TCN}.},
	number = {{arXiv}:1803.01271},
	publisher = {{arXiv}},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	urldate = {2023-08-23},
	date = {2018-04-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.01271 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:/home/clement/Zotero/storage/TM3UYTWZ/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	volume = {30},
	pages = {5998--6008},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-08-23},
	date = {2017},
	file = {Attention Is All You Need.pdf:/home/clement/Zotero/storage/QEQIBNKM/Attention Is All You Need.pdf:application/pdf},
}

@article{lai_recurrent_2015,
	title = {Recurrent Convolutional Neural Networks for Text Classification},
	volume = {29},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/9513},
	doi = {10.1609/aaai.v29i1.9513},
	abstract = {Text classiﬁcation is a foundational task in many {NLP} applications. Traditional text classiﬁers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classiﬁcation without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classiﬁcation to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	urldate = {2023-08-23},
	date = {2015-02-19},
	langid = {english},
	file = {Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text C.pdf:/home/clement/Zotero/storage/CP2DU5RB/Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text C.pdf:application/pdf},
}

@article{hiester_file_2018,
	title = {File Fragment Classification Using Neural Networks with Lossless Representations},
	abstract = {This study explores the use of neural networks as universal models for classifying ﬁle fragments. This approach diﬀers from previous work in its lossless feature representation, with fragments’ bits as direct input, and its use of feedforward, recurrent, and convolutional networks as classiﬁers, whereas previous work has only tested feedforward networks. Due to the study’s exploratory nature, the models were not directly evaluated in a practical setting; rather, easily reproducible experiments were performed to attempt to answer the intial question of whether this approach is worthwhile to pursue further, especially due to its high computational cost. The experiments tested classiﬁcation of fragments of homogeneous ﬁle types as an idealized case, rather than using a realistic set of types, because the types of interest are highly application-dependent. The recurrent networks achieved 98 percent accuracy in distinguishing 4 ﬁle types, suggesting that this approach may be capable of yielding models with sufﬁcient performance for practical applications. The potential applications depend mainly on the model performance gains achievable by future work but include binary mapping, deep packet inspection, and ﬁle carving.},
	journaltitle = {East Tennessee State University},
	author = {Hiester, Luke},
	urldate = {2023-08-21},
	date = {2018-05},
	langid = {english},
	file = {Hiester - File Fragment Classification Using Neural Networks.pdf:/home/clement/Zotero/storage/J5IUYN87/Hiester - File Fragment Classification Using Neural Networks.pdf:application/pdf},
}

@article{sentanoe_sshkex_2022,
	title = {{SSHkex}: Leveraging virtual machine introspection for extracting {SSH} keys and decrypting {SSH} network traffic},
	volume = {40},
	issn = {26662817},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666281722000063},
	doi = {10.1016/j.fsidi.2022.301337},
	shorttitle = {{SSHkex}},
	abstract = {Nowadays, many users are using an encrypted channel to communicate with a remote resource server. Such a channel provides a high degree of privacy and conﬁdentiality. Secure Shell ({SSH}) is one of the most commonly used methods to connect to a server remotely. {SSH} provides privacy and conﬁdentiality by encrypting network trafﬁc between the client and the server. The encryption makes the learning process of malicious activities over {SSH} is challenging, especially by just analyzing the network trafﬁc. To overcome the problem, we can leverage Virtual machine introspection ({VMI}). {VMI} allows direct memory access of a virtual machine ({VM}) including accessing data of an {SSH} process. However, the current prototype suffers from high overhead since it extracts every single plain text {SSH} network payload from memory and the extraction process requires the virtual machine ({VM}) to be momentarily paused. In this paper, we introduce {SSHkex}, a tool that also leverages {VMI} to extracts {SSH}'s session keys from a server's memory. Our approach only needs to pause the {VM} twice to extract the session keys for each {SSH} session and does passive network monitoring where does not have any noticeable impact on the ongoing connection. To use {SSHkex}, zero modiﬁcation needs to be done to the server. Thus, it is suitable for intrusion detection systems and high-interaction honeypot where the server shall not be modiﬁed.},
	pages = {301337},
	journaltitle = {Forensic Science International: Digital Investigation},
	shortjournal = {Forensic Science International: Digital Investigation},
	author = {Sentanoe, Stewart and Reiser, Hans P.},
	urldate = {2023-08-17},
	date = {2022-04},
	langid = {english},
	file = {Sentanoe et Reiser - 2022 - SSHkex Leveraging virtual machine introspection f.pdf:/home/clement/Zotero/storage/GBJCY9P8/Sentanoe et Reiser - 2022 - SSHkex Leveraging virtual machine introspection f.pdf:application/pdf},
}

@misc{fellicious_smartkex_2022,
	title = {{SmartKex}: Machine Learning Assisted {SSH} Keys Extraction From The Heap Dump},
	url = {http://arxiv.org/abs/2209.05243},
	shorttitle = {{SmartKex}},
	abstract = {Digital forensics is the process of extracting, preserving, and documenting evidence in digital devices. A commonly used method in digital forensics is to extract data from the main memory of a digital device. However, the main challenge is identifying the important data to be extracted. Several pieces of crucial information reside in the main memory, like usernames, passwords, and cryptographic keys such as {SSH} session keys. In this paper, we propose {SmartKex}, a machine-learning assisted method to extract session keys from heap memory snapshots of an {OpenSSH} process. In addition, we release an openly available dataset and the corresponding toolchain for creating additional data. Finally, we compare {SmartKex} with naive brute-force methods and empirically show that {SmartKex} can extract the session keys with high accuracy and high throughput. With the provided resources, we intend to strengthen the research on the intersection between digital forensics, cybersecurity, and machine learning.},
	number = {{arXiv}:2209.05243},
	publisher = {{arXiv}},
	author = {Fellicious, Christofer and Sentanoe, Stewart and Granitzer, Michael and Reiser, Hans P.},
	urldate = {2023-08-17},
	date = {2022-09-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.05243 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Fellicious et al. - 2022 - SmartKex Machine Learning Assisted SSH Keys Extra.pdf:/home/clement/Zotero/storage/FVESHJZY/Fellicious et al. - 2022 - SmartKex Machine Learning Assisted SSH Keys Extra.pdf:application/pdf},
}

@misc{huang_character-level_2016,
	title = {Character-level Convolutional Network for Text Classification Applied to Chinese Corpus},
	url = {http://arxiv.org/abs/1611.04358},
	abstract = {This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem.},
	number = {{arXiv}:1611.04358},
	publisher = {{arXiv}},
	author = {Huang, Weijie and Wang, Jun},
	urldate = {2023-08-17},
	date = {2016-11-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.04358 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Huang et Wang - 2016 - Character-level Convolutional Network for Text Cla.pdf:/home/clement/Zotero/storage/SCGHI52Z/Huang et Wang - 2016 - Character-level Convolutional Network for Text Cla.pdf:application/pdf},
}

@online{gite_how_2008,
	title = {How To Reuse {SSH} Connection To Speed Up Remote Login Process Using Multiplexing},
	url = {https://www.cyberciti.biz/faq/linux-unix-reuse-openssh-connection/},
	abstract = {Explains how to reuse existing {OpenSSH} connections using multiplexing to speed up an ssh login connection procedure on Linux/Unix/{macOS}/*{BSD}.},
	titleaddon = {{nixCraft}},
	author = {Gite, Vivek},
	urldate = {2022-10-21},
	date = {2008-08-20},
	langid = {american},
	file = {Snapshot:/home/clement/Zotero/storage/J2TCZAJP/linux-unix-reuse-openssh-connection.html:text/html},
}

@article{macqueen_methods_1967,
	title = {{SOME} {METHODS} {FOR} {CLASSIFICATION} {AND} {ANALYSIS} {OF} {MULTIVARIATE} {OBSERVATIONS}},
	volume = {{VOL}. 5.1},
	journaltitle = {{MULTIVARIATE} {OBSERVATIONS}},
	shortjournal = {Berkeley Symp. on Math. Statist. and Prob},
	author = {Macqueen, J},
	date = {1967},
	langid = {english},
	file = {Macqueen - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MU.pdf:/home/clement/Zotero/storage/459XE8DU/Macqueen - SOME METHODS FOR CLASSIFICATION AND ANALYSIS OF MU.pdf:application/pdf},
}

@article{ester_density-based_1996,
	title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
	abstract = {Clustering algorithms are attractive for the task of class identiﬁcation in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efﬁciency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm {DBSCAN} relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. {DBSCAN} requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efﬁciency of {DBSCAN} using synthetic data and real data of the {SEQUOIA} 2000 benchmark. The results of our experiments demonstrate that (1) {DBSCAN} is signiﬁcantly more effective in discovering clusters of arbitrary shape than the well-known algorithm {CLARANS}, and that (2) {DBSCAN} outperforms {CLARANS} by a factor of more than 100 in terms of efﬁciency.},
	journaltitle = {{KDD}-96},
	author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jörg and Xu, Xiaowei},
	date = {1996-08-02},
	langid = {english},
	file = {Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf:/home/clement/Zotero/storage/M36EAE5Y/Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf:application/pdf},
}

@misc{von_luxburg_tutorial_2007,
	title = {A Tutorial on Spectral Clustering},
	url = {http://arxiv.org/abs/0711.0189},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved eﬃciently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the ﬁrst glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe diﬀerent graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several diﬀerent approaches. Advantages and disadvantages of the diﬀerent spectral clustering algorithms are discussed.},
	number = {{arXiv}:0711.0189},
	publisher = {{arXiv}},
	author = {von Luxburg, Ulrike},
	urldate = {2023-09-05},
	date = {2007-11-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {0711.0189 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:/home/clement/Zotero/storage/5WH2FANC/von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:application/pdf},
}

@misc{fellicious_machine_2022,
	title = {Machine Learning Assisted {SSH} Keys Extraction From The Heap Dump},
	rights = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/6537904},
	doi = {10.5281/ZENODO.6537904},
	abstract = {This dataset contains heap dump of {OpenSSH} that contains session keys. On the performance test data, we also include the {PCAP} file that contains the encrypted {SSH} network traffic. With the correct session keys, it can be decrypted.},
	version = {0.1},
	publisher = {Zenodo},
	author = {Fellicious, Christofer and Sentanoe, Stewart and Granitzer, Michael and Reiser, Hans P.},
	urldate = {2023-09-06},
	date = {2022-08-15},
	langid = {english},
	keywords = {memory heap dump, openssh, session keys},
}
